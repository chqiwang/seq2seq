# BIOHAZARD -- DO NOT EDIT THIS FILE

# training
learning_rate: 0.5       # initial learning rate
max_gradient_norm: 5.0   # clip gradients to this norm
dropout_rate: 0.0        # dropout rate applied to the RNN units
max_train_size: 0        # maximum size of the training data (0 for unlimited)
max_dev_size: 0          # maximum size of the dev data
steps_per_checkpoint: 1000   # number of updates between each checkpoint
steps_per_eval: 4000     # number of updates between each BLEU eval (on dev set)
eval_burn_in: 0          # minimum number of steps before starting BLEU eval
max_steps: 0             # maximum number of updates before stopping
keep_best: 4             # number of best checkpoints to keep
feed_previous: 0.0       # randomly feed previous output instead of groundtruth to decoder during training
optimizer: 'sgd'         # 'sgd', 'adadelta', 'adagrad' or 'adam'
read_ahead: 10           # number of batches to read ahead and sort

# model (each one of these settings can be defined specifically in `encoders` and `decoder`, or generally here)
attention: True          # use an attention mechanism
batch_size: 80           # training batch size
cell_size: 1000          # size of the RNN cells
embedding_size: 620      # size of the embeddings
attn_size: 1000          # size of the attention layer
layers: 1                # number of RNN layers per encoder and decoder
bidir: True              # use bidirectional encoders
vocab_size: 0            # number of symbols of each encoder and decoder (0: size of vocab files)
use_lstm: False          # use LSTM units instead of GRU units
weight_scale: null       # if not null, initialize all weights to a normal distribution with this stdev

# data
data_dir: data           # directory containing the training data
model_dir: model         # directory where the model will be saved (checkpoints and eval outputs)
train_prefix: train      # name of the training corpus
script_dir: scripts      # directory where the scripts are kepts (in particular the scoring scripts)
dev_prefix: dev          # names of the development corpora
vocab_prefix: vocab      # name of the vocabulary files
checkpoints: []          # list of checkpoints to load (in this specific order) after main checkpoint

# decoding
remove_unk: False        # remove UNK symbols from the decoder output
beam_size: 1             # beam size for decoding (decoder is greedy by default)
output: null             # output file for decoding (writes to standard output by default)
max_output_len: 50       # maximum length of the sequences generated by the decoder (strongly affects decoding speed)

# general
gpu_id: 0                # index of the GPU to use
no_gpu: False            # don't use any GPU
allow_growth: True       # allow GPU memory allocation to change during runtime
mem_fraction: 1.0        # maximum fraction of GPU memory to use
log_file: null           # log to this file instead of standard output
parallel_iterations: 16  # parameter of Tensorflow's while_loop
swap_memory: True        # parameter of Tensorflow's while_loop

encoders:
    name: fr

decoder:
    name: en
